import asyncio
import logging
from pathlib import Path
from datetime import datetime

from src.evaluations.core.eval import (
    DataLoader,
    AzureOpenAIClient,
    AsyncExperimentRunner,
)
from src.utils.log import logger

from src.evaluations.core.experiments.eai.evaluators import (
    MemoryTestLLMGuidedConversation,
    SearchCallsAfterSecondQuestionEvaluator,
    CriticalFactAccuracyEvaluator,
    HallucinationFlagEvaluator,
    TokenUsageTotalEvaluator,
)
from src.evaluations.core.experiments.eai.evaluators.prompts import (
    prompt_data,
)

EXPERIMENT_DATA_PATH = Path(__file__).parent / "data"

# ========================================
# CONFIGURA√á√ÉO DO EXPERIMENTO
# ========================================

# Escolha qual vers√£o rodar: "v1" ou "v2"
VERSION = "v1"  # Altere aqui para "v2" quando quiser testar mem√≥ria restrita

# IDs dos reasoning engines (configure com seus IDs reais ou use None para padr√£o)
REASONING_ENGINE_IDS = {
    "v1": None,  # Mem√≥ria Total - None usa o engine padr√£o
    "v2": "3875545391445311488",  # Mem√≥ria Restrita - ou None para padr√£o
}

# Descri√ß√µes autom√°ticas por vers√£o
DESCRIPTIONS = {
    "v1": "Full memory - includes tool_call and tool_response",
    "v2": "Restricted memory - only user_message, reasoning, ia_response",
}

# ========================================


async def run_experiment(
    version: str = "v1",
    reasoning_engine_id: str = None,
    description: str = "",
):
    """
    Executa o experimento de teste de mem√≥ria para avaliar o impacto da
    reten√ß√£o de tool_call e tool_response no hist√≥rico de conversa.
    
    Args:
        version: Vers√£o da configura√ß√£o ('v1' = mem√≥ria total, 'v2' = mem√≥ria restrita)
        reasoning_engine_id: ID do reasoning engine a ser usado
        description: Descri√ß√£o adicional para o experimento
    """
    logger.info(f"--- Configurando o Experimento de Mem√≥ria - {version.upper()} ---")

    # --- 1. Configura√ß√£o do DataLoader ---
    loader = DataLoader(
        source="https://docs.google.com/spreadsheets/d/1VPnJSf9puDgZ-Ed9MRkpe3Jy38nKxGLp7O9-ydAdm98/edit?gid=969839054#gid=969839054",  # Multi-turn memory test
        # number_rows=3,  # Descomente para testar com menos linhas
        id_col="id_original",
        prompt_col="initial_user_message",
        dataset_name="Multi-Turn Memory Test",
        dataset_description="Dataset de avalia√ß√£o de mem√≥ria multi-turno com depend√™ncia factual",
        metadata_cols=[
            "tema",
            "secondary_user_message",
            "dependency_type",
            "critical_fact_reused",
            "fact_type",
            "memory_sensitivity_level",
        ],
        # upload_to_bq=False,  # Descomente para n√£o fazer upload ao BigQuery
    )
    logger.info(
        f"‚úÖ DataLoader configurado para o dataset: '{loader.get_dataset_config()['dataset_name']}'"
    )

    # --- 2. Configura√ß√£o do Cliente Judge (LLM) ---
    judge_client = AzureOpenAIClient(model_name="gpt-4o")
    # judge_client = GeminiAIClient(model_name="gemini-1.5-flash-latest")

    # --- 3. Defini√ß√£o da Su√≠te de Avalia√ß√£o ---
    evaluators_to_run = [
        MemoryTestLLMGuidedConversation(judge_client),
        SearchCallsAfterSecondQuestionEvaluator(judge_client),
        CriticalFactAccuracyEvaluator(judge_client),
        HallucinationFlagEvaluator(judge_client),
        TokenUsageTotalEvaluator(judge_client),
    ]

    evaluator_names = [e.name for e in evaluators_to_run]
    logger.info(f"‚úÖ Su√≠te de avalia√ß√µes configurada para rodar: {evaluator_names}")

    # --- 4. Coleta de Metadados do Experimento ---
    judges_prompts = {
        evaluator.name: evaluator.PROMPT_TEMPLATE
        for evaluator in evaluators_to_run
        if hasattr(evaluator, "PROMPT_TEMPLATE")
    }

    metadata = {
        "version": version,
        "version_description": description,
        "system_prompt": prompt_data["prompt"],
        "prompt_version": prompt_data["version"],
        "judge_model": judge_client.model_name,
        "judges_prompts": judges_prompts,
        "experiment_type": "memory_test",
        "memory_config": {
            "v1": "Full memory (includes tool_call and tool_response)",
            "v2": "Restricted memory (only user_message, reasoning, ia_response)",
        }.get(version, "Unknown"),
    }

    # --- 5. Configura√ß√£o e Execu√ß√£o do Runner ---
    MAX_CONCURRENCY = 20

    runner = AsyncExperimentRunner(
        experiment_name=f"eai-memory-{version}-{datetime.now().strftime('%Y-%m-%d')}",
        experiment_description=description or f"Memory test - {version}",
        metadata=metadata,
        evaluators=evaluators_to_run,
        max_concurrency=MAX_CONCURRENCY,
        # upload_to_bq=False,  # Descomente para n√£o fazer upload ao BigQuery
        output_dir=EXPERIMENT_DATA_PATH,
        rate_limit_requests_per_minute=1000,
        reasoning_engine_id=reasoning_engine_id,
    )
    logger.info(f"‚úÖ Runner pronto para o experimento: '{runner.experiment_name}'")
    
    # Executa o experimento
    await runner.run(loader)


if __name__ == "__main__":
    """
    Para rodar o experimento:
    
    1. Configure a VERSION no topo do arquivo ("v1" ou "v2")
    2. Configure os REASONING_ENGINE_IDS com seus IDs reais
    3. Execute: uv run src/evaluations/core/experiments/eai/run_memory_experiment.py
    """
    
    # Valida a configura√ß√£o
    if VERSION not in ["v1", "v2"]:
        print(f"‚ùå Erro: VERSION deve ser 'v1' ou 'v2', mas est√° configurada como '{VERSION}'")
        print("   Edite a vari√°vel VERSION no topo do arquivo.")
        exit(1)
    
    # Pega o reasoning_engine_id (pode ser None para usar o padr√£o)
    reasoning_engine_id = REASONING_ENGINE_IDS.get(VERSION)
    
    description = DESCRIPTIONS.get(VERSION, f"Memory test - {VERSION}")
    
    # Exibe a configura√ß√£o atual
    print("=" * 60)
    print(f"üöÄ EXPERIMENTO DE MEM√ìRIA - {VERSION.upper()}")
    print("=" * 60)
    print(f"Vers√£o: {VERSION}")
    print(f"Descri√ß√£o: {description}")
    print(f"Reasoning Engine ID: {reasoning_engine_id or 'Padr√£o (None)'}")
    print("=" * 60)
    print()
    
    try:
        asyncio.run(run_experiment(
            version=VERSION,
            reasoning_engine_id=reasoning_engine_id,
            description=description
        ))
    except Exception as e:
        logging.getLogger(__name__).error(
            f"Ocorreu um erro fatal durante a execu√ß√£o do experimento: {e}",
            exc_info=True,
        )
